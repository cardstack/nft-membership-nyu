{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a863de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashasvi/opt/anaconda3/envs/itp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%run Imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad0eadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 19:35:35 WARN Utils: Your hostname, Yashasvis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.232 instead (on interface en0)\n",
      "23/04/18 19:35:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/18 19:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "spark = SparkSession.builder.appName(\"Rank\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c7a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1=spark.read.parquet(\"/Users/yashasvi/Documents/CleanDuneData/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"Load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381768bf",
   "metadata": {},
   "source": [
    "Main Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beba8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.sql(\"SELECT * FROM ( SELECT n1.buyer, n1.original_currency, n1.nft_token_id, n1.nft_contract_address, n1.original_amount as Bought, n2.original_amount as Sold, n1.block_time as Bought_Timestamp, n2.block_time as Sold_Timestamp,n1.usd_amount as Bought_amount, n2.usd_amount as Sold_amount, (n2.usd_amount - n1.usd_amount) as net, n1.platform as n1platform, n2.platform as n2platform FROM Load n1, Load n2 where n1.buyer = n2.seller AND  n2.block_time > n1.block_time AND  n1.nft_token_id is not null AND n1.nft_token_id = n2.nft_token_id AND n1.nft_contract_address = n2.nft_contract_address) holdings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81195c",
   "metadata": {},
   "source": [
    "# Ranking 1 - average amount spent (here profit , loss automatically done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de84dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_avg = df_all.groupBy(\"buyer\").agg(avg(\"net\").alias(\"avg_spent\"))\n",
    "# score1 based on avg_spent\n",
    "df_rule_avg_spent_pd=df_avg.toPandas()\n",
    "\n",
    "# Calculate percentiles for customers with avg_spent > 0\n",
    "df_positive = df_rule_avg_spent_pd[df_rule_avg_spent_pd['avg_spent'] > 0]\n",
    "percentiles = df_positive['avg_spent'].rank(pct=True)\n",
    "df_positive['Score1'] = percentiles * 100\n",
    "\n",
    "# Set score to 0 for customers with avg_spent < 0\n",
    "df_negative = df_rule_avg_spent_pd[df_rule_avg_spent_pd['avg_spent'] <= 0]\n",
    "df_negative['Score1'] = 0\n",
    "\n",
    "# Combine positive and negative dataframes\n",
    "df_combined = pd.concat([df_positive, df_negative], axis=0)\n",
    "df_rule_avg_spent=spark.createDataFrame(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49396215",
   "metadata": {},
   "source": [
    "# Ranking 2 - number of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b219fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:36:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_count = df_all.groupBy(\"buyer\").agg(count(\"*\").alias(\"number_txns\"))\n",
    "\n",
    "# score2 based on number_txns\n",
    "df_rule_number_txns_pd=df_count.toPandas()\n",
    "\n",
    "percentiles = df_rule_number_txns_pd['number_txns'].rank(pct=True)\n",
    "df_rule_number_txns_pd['Score2'] = percentiles * 100\n",
    "df_rule_number_txns=spark.createDataFrame(df_rule_number_txns_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a6eb5",
   "metadata": {},
   "source": [
    "# Diversity Ranking 3 - Number of Nft contract address held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797c8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_num_nftcontract = df_all.groupBy(\"buyer\").agg(countDistinct(\"nft_contract_address\").alias(\"num_nftcontract\"))\n",
    "# score3 based on num_nftcontract\n",
    "df_rule_num_nftcontract_pd=df_num_nftcontract.toPandas()\n",
    "\n",
    "percentiles = df_rule_num_nftcontract_pd['num_nftcontract'].rank(pct=True)\n",
    "df_rule_num_nftcontract_pd['Score3'] = percentiles * 100\n",
    "#converting to sparkdf\n",
    "df_rule_num_nftcontract=spark.createDataFrame(df_rule_num_nftcontract_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349eb336",
   "metadata": {},
   "source": [
    "# Diversity Ranking 4 - Number of unique currencies transaction done in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0291f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_currency = df_all.groupBy(\"buyer\").agg(countDistinct(\"original_currency\").alias(\"num_currency\"))\n",
    "# score4 based on num_currency\n",
    "df_rule_num_currency_pd=df_currency.toPandas()\n",
    "percentiles = df_rule_num_currency_pd['num_currency'].rank(pct=True)\n",
    "df_rule_num_currency_pd['Score4'] = percentiles * 100\n",
    "#converting back to spark df\n",
    "df_rule_num_currency=spark.createDataFrame(df_rule_num_currency_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7b98e",
   "metadata": {},
   "source": [
    "# Diversity Ranking 5 - Average Duration (in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed31f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.withColumn(\"duration\", (unix_timestamp(col(\"Sold_Timestamp\")) - unix_timestamp(col(\"Bought_Timestamp\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d650d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 19:38:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_avg_duration = df_all.groupBy(\"buyer\").agg(avg(\"duration\").alias(\"avg_duration\"))\n",
    "# score4 based on num_currency\n",
    "df_rule_avg_duration_pd=df_avg_duration.toPandas()\n",
    "\n",
    "percentiles = df_rule_avg_duration_pd['avg_duration'].rank(pct=True)\n",
    "df_rule_avg_duration_pd['Score5'] = percentiles * 100\n",
    "#converting to sparkdf\n",
    "df_rule_avg_duration=spark.createDataFrame(df_rule_avg_duration_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f43872",
   "metadata": {},
   "source": [
    "# Ranking 6 - Frequency of each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b400165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df_all = df_all.withColumn(\"date\", split(\"Bought_Timestamp\", \" \")[0])\n",
    "df_all = df_all.withColumn(\"time\", split(\"Bought_Timestamp\", \" \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f954a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_frequency = df_all.groupBy(\"buyer\").agg(countDistinct(\"time\").alias(\"frequency\"))\n",
    "df_rule_frequency_pd = df_frequency.toPandas()\n",
    "\n",
    "percentiles = df_rule_frequency_pd['frequency'].rank(pct=True)\n",
    "df_rule_frequency_pd['Score6'] = percentiles * 100\n",
    "#converting to sparkdf\n",
    "df_rule_frequency=spark.createDataFrame(df_rule_frequency_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3fc65",
   "metadata": {},
   "source": [
    "# Feature Extraction for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81f5897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined_scores = df_rule_avg_spent.join(df_rule_number_txns, on='buyer', how='inner').join(df_rule_num_nftcontract, on='buyer', how='inner').join(df_rule_num_currency, on='buyer', how='inner').join(df_rule_avg_duration, on='buyer', how='inner').join(df_rule_frequency, on='buyer', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_scores.write.parquet(\"/Users/yashasvi/Documents/ModelScores/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08f2dc",
   "metadata": {},
   "source": [
    "# Time Series Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e77bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_rule_avg_spent.join(df_rule_number_txns, on='buyer', how='inner').join(df_rule_num_nftcontract, on='buyer', how='inner').join(df_rule_num_currency, on='buyer', how='inner').join(df_rule_frequency, on='buyer', how='inner').join(df_rule_avg_duration, on='buyer', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e05481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering records for past 1 year and past 2 years\n",
    "df_all_filtered_1yearago = df_all.filter(df_all['Bought_Timestamp'] < date_sub(current_date(), 365))\n",
    "df_all_filtered_2yearsago = df_all.filter(df_all['Bought_Timestamp'] < date_sub(current_date(), 365*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f944576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_spent_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(avg(\"net\").alias(\"avg_spent_1\"))\n",
    "df_avg_spent_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(avg(\"net\").alias(\"avg_spent_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fa847f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwise(df_avg,colname,scorename):\n",
    "    df_rule_avg_spent_pd=df_avg.toPandas()\n",
    "    df_positive = df_rule_avg_spent_pd[df_rule_avg_spent_pd[colname] > 0]\n",
    "    percentiles = df_positive[colname].rank(pct=True)\n",
    "    df_positive[scorename] = percentiles * 100\n",
    "    df_negative = df_rule_avg_spent_pd[df_rule_avg_spent_pd[colname] <= 0]\n",
    "    df_negative[scorename] = 0\n",
    "    df_combined = pd.concat([df_positive, df_negative], axis=0)\n",
    "    df_spark=spark.createDataFrame(df_combined)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4900529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rule_avg_1 = yearwise(df_avg_spent_1, 'avg_spent_1','Score1_1')\n",
    "df_rule_avg_2 = yearwise(df_avg_spent_2, 'avg_spent_2','Score1_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab630fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(count(\"*\").alias(\"number_txns_1\"))\n",
    "df_count_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(count(\"*\").alias(\"number_txns_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d915c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwiseranking2(df_count, colname, scorename):\n",
    "    df_rule_number_txns_pd=df_count.toPandas()\n",
    "    percentiles = df_rule_number_txns_pd[colname].rank(pct=True)\n",
    "    df_rule_number_txns_pd[scorename] = percentiles * 100\n",
    "    df_rule_number_txns=spark.createDataFrame(df_rule_number_txns_pd)\n",
    "    return df_rule_number_txns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5b95d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rule_number_txns_1 = yearwiseranking2(df_count_1, 'number_txns_1','Score2_1')\n",
    "df_rule_number_txns_2 = yearwiseranking2(df_count_2, 'number_txns_2','Score2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eede978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_nftcontract_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(countDistinct(\"nft_contract_address\").alias(\"num_nftcontract_1\"))\n",
    "df_num_nftcontract_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(countDistinct(\"nft_contract_address\").alias(\"num_nftcontract_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "046d5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwiseranking3(df_num_nftcontract, colname, scorename):\n",
    "    df_rule_num_nftcontract_pd=df_num_nftcontract.toPandas()\n",
    "    percentiles = df_rule_num_nftcontract_pd[colname].rank(pct=True)\n",
    "    df_rule_num_nftcontract_pd[scorename] = percentiles * 100\n",
    "    df_rule_num_nftcontract=spark.createDataFrame(df_rule_num_nftcontract_pd)\n",
    "    return df_rule_num_nftcontract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a2cdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rule_num_nftcontract_1 = yearwiseranking3(df_num_nftcontract_1, 'num_nftcontract_1','Score3_1')\n",
    "df_rule_num_nftcontract_2 = yearwiseranking3(df_num_nftcontract_2, 'num_nftcontract_2','Score3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f9a8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_currency_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(countDistinct(\"original_currency\").alias(\"num_currency_1\"))\n",
    "df_currency_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(countDistinct(\"original_currency\").alias(\"num_currency_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfcf3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwiseranking4(df_currency, colname, scorename):\n",
    "    df_rule_num_currency_pd=df_currency.toPandas()\n",
    "    percentiles = df_rule_num_currency_pd[colname].rank(pct=True)\n",
    "    df_rule_num_currency_pd[scorename] = percentiles * 100\n",
    "    df_rule_num_currency=spark.createDataFrame(df_rule_num_currency_pd)\n",
    "    return df_rule_num_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6408a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rule_num_currency_1 = yearwiseranking4(df_currency_1, 'num_currency_1','Score4_1')\n",
    "df_rule_num_currency_2 = yearwiseranking4(df_currency_2, 'num_currency_2','Score4_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6d36def",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_duration_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(avg(\"duration\").alias(\"avg_duration_1\"))\n",
    "df_avg_duration_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(avg(\"duration\").alias(\"avg_duration_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9e2b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwiseranking5(df_avg_duration, colname, scorename):\n",
    "    df_rule_avg_duration_pd=df_avg_duration.toPandas()\n",
    "    percentiles = df_rule_avg_duration_pd[colname].rank(pct=True)\n",
    "    df_rule_avg_duration_pd[scorename] = percentiles * 100\n",
    "    df_rule_avg_duration=spark.createDataFrame(df_rule_avg_duration_pd)\n",
    "    return df_rule_avg_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e5dd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rule_avg_duration_1 = yearwiseranking5(df_avg_duration_1, 'avg_duration_1','Score5_1')\n",
    "df_rule_avg_duration_2 = yearwiseranking5(df_avg_duration_2, 'avg_duration_2','Score5_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c17b8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency_1 = df_all_filtered_1yearago.groupBy(\"buyer\").agg(countDistinct(\"time\").alias(\"frequency_1\"))\n",
    "df_frequency_2 = df_all_filtered_2yearsago.groupBy(\"buyer\").agg(countDistinct(\"time\").alias(\"frequency_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a731a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearwiseranking6(df_frequency, colname, scorename):\n",
    "    df_rule_frequency_pd = df_frequency.toPandas()\n",
    "    percentiles = df_rule_frequency_pd[colname].rank(pct=True)\n",
    "    df_rule_frequency_pd[scorename] = percentiles * 100\n",
    "    df_rule_frequency=spark.createDataFrame(df_rule_frequency_pd)\n",
    "    return df_rule_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db0272ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rule_frequency_1 = yearwiseranking6(df_frequency_1, 'frequency_1','Score6_1')\n",
    "df_rule_frequency_2 = yearwiseranking6(df_frequency_2, 'frequency_2','Score6_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "431b2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datframe = joined_df.join(df_rule_avg_1, on='buyer', how='outer').join(df_rule_avg_2, on='buyer', how='outer').join(df_rule_number_txns_1, on='buyer', how='outer').join(df_rule_number_txns_2, on='buyer', how='outer').join(df_rule_num_nftcontract_1, on='buyer', how='outer').join(df_rule_num_nftcontract_2, on='buyer', how='outer').join(df_rule_num_currency_1, on='buyer', how='outer').join(df_rule_num_currency_2, on='buyer', how='outer').join(df_rule_avg_duration_1, on='buyer', how='outer').join(df_rule_avg_duration_2, on='buyer', how='outer').join(df_rule_frequency_1, on='buyer', how='outer').join(df_rule_frequency_2, on='buyer', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b844215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/18 22:11:07 WARN TaskSetManager: Stage 558 contains a task of very large size (1435 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:07 WARN TaskSetManager: Stage 559 contains a task of very large size (1280 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:08 WARN TaskSetManager: Stage 560 contains a task of very large size (1280 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:08 WARN TaskSetManager: Stage 561 contains a task of very large size (1280 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:08 WARN TaskSetManager: Stage 562 contains a task of very large size (1280 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:08 WARN TaskSetManager: Stage 563 contains a task of very large size (1435 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:08 WARN TaskSetManager: Stage 564 contains a task of very large size (1109 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:09 WARN TaskSetManager: Stage 566 contains a task of very large size (1048 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:09 WARN TaskSetManager: Stage 568 contains a task of very large size (1048 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:09 WARN TaskSetManager: Stage 570 contains a task of very large size (1048 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:09 WARN TaskSetManager: Stage 572 contains a task of very large size (1109 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:10 WARN TaskSetManager: Stage 574 contains a task of very large size (1048 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/04/18 22:11:17 WARN MemoryManager: Total allocation exceeds 95.00% (910,478,529 bytes) of heap memory\n",
      "Scaling row group sizes to 96.91% for 7 writers\n",
      "23/04/18 22:11:17 WARN MemoryManager: Total allocation exceeds 95.00% (910,478,529 bytes) of heap memory\n",
      "Scaling row group sizes to 84.79% for 8 writers\n",
      "23/04/18 22:11:19 WARN MemoryManager: Total allocation exceeds 95.00% (910,478,529 bytes) of heap memory\n",
      "Scaling row group sizes to 96.91% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_datframe.write.parquet(\"/Users/yashasvi/Documents/yearwisedistribution/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:itp]",
   "language": "python",
   "name": "conda-env-itp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
