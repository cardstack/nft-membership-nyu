{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d88ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb17215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37115965",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d04c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 17:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/22 17:31:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Transformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b8b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1=spark.read.parquet(\"/Users/abhishek/Documents/load/Parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b50f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"Load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac75d2",
   "metadata": {},
   "source": [
    "# Removing bot activities - Activity 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3e9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1 = spark.sql(\"SELECT * FROM Load Where buyer NOT IN \\\n",
    "                       (SELECT buyer FROM (SELECT buyer, count(buyer) as counter from Load \\\n",
    "                       GROUP BY buyer, (EXTRACT(HOUR FROM block_time)), (EXTRACT(MINUTE FROM block_time))) temp\\\n",
    "                       WHERE counter > 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3584c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_1.createOrReplaceTempView(\"Load1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd98ed",
   "metadata": {},
   "source": [
    "# Removing bot activities - Activity 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38230db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage_2 = spark.sql(\"SELECT * FROM Load1 Where buyer NOT IN \\\n",
    "                       (SELECT buyer FROM(SELECT a.buyer, a.original_amount, b.original_amount,(b.block_time - a.block_time) time_gap \\\n",
    "                        FROM Load1 a INNER JOIN Load1 b \\\n",
    "                        ON (a.buyer = b.seller AND a.block_time < b.block_time AND a.nft_token_id = b.nft_token_id )) \\\n",
    "                        WHERE (EXTRACT(MINUTES FROM time_gap)) < 1 AND (EXTRACT(HOURS FROM time_gap)) = 0 AND \\\n",
    "                       (EXTRACT(DAYS FROM time_gap)) = 0 ORDER BY time_gap ASC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9706e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 04:57:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/21 04:57:29 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/21 04:57:31 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_stage_2.write.parquet(\"/Users/abhishek/Documents/Cleansed/Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3ad7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
