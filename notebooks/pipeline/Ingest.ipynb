{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4727e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dunebuggy\n",
    "from dunebuggy import Dune\n",
    "\n",
    "dune = Dune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e910df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3adcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87e4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 17:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Dune\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9715da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10-16-245-43.dynapool.wireless.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dune</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9f20fc26d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694ccc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dune_query = {'90': 2201677, '91': 2203423, '92': 2203427, '93': 2203429,'94': 2203430,'95': 2203432,'96': 2203524,'97': 2203528,'98': 2203530,\n",
    "                  '99': 2203531,'d0': 2203532,'d1': 2203534,'d2': 2203571,'d3': 2203573,'d4': 2203575,'d5': 2203576,'d6': 2203577,'d7': 2203580,\n",
    "                 'd8': 2203581,'d9': 2203582,'e0': 2203608,'e1': 2203614,'e2': 2203616,'e3': 2203617,'e4': 2203618,'e5': 2203620,'e6': 2203622,\n",
    "                 'e7': 2203623,'e8': 2203624,'e9': 2203625,'f0': 2203758,'f1': 2203761,'f2': 2203762,'f3': 2203767,'f4': 2203770,'f5': 2203771,\n",
    "                 'f6': 2203772,'f7': 2203774,'f8': 2203775,'f9': 2203777}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a3140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSchema = StructType((\n",
    "    StructField(\"block_number\",IntegerType(),True),\n",
    "    StructField(\"block_time\",TimestampType(),True),\n",
    "    StructField(\"buyer\",StringType(),True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"evt_type\", StringType(), True),\n",
    "    StructField(\"nft_contract_address\", StringType(), True),\n",
    "    StructField(\"nft_contract_addresses_array\", StringType(), True),\n",
    "    StructField(\"nft_project_name\", StringType(), True),\n",
    "    StructField(\"nft_token_id\", StringType(), True),\n",
    "    StructField(\"nft_token_ids_array\", StringType(), True),\n",
    "    StructField(\"number_of_items\", StringType(), True),\n",
    "    StructField(\"original_amount\", StringType(), True),\n",
    "    StructField(\"original_currency\", StringType(), True),\n",
    "    StructField(\"platform\", StringType(), True),\n",
    "    StructField(\"seller\", StringType(), True),\n",
    "    StructField(\"trade_id\", IntegerType(), True),\n",
    "    StructField(\"trade_type\", StringType(), True),\n",
    "    StructField(\"usd_amount\", FloatType(), True),\n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e996df",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dune.fetch_query(2201677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cd7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = a.df[['block_number', 'block_time', 'buyer', 'category', 'evt_type', 'nft_contract_address', 'nft_contract_addresses_array',\n",
    "               'nft_project_name', 'nft_token_id', 'nft_token_ids_array', 'number_of_items', 'original_amount', 'original_currency',\n",
    "               'platform', 'seller', 'trade_id', 'trade_type', 'usd_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1319975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/7__pfpmx6ssfs894hh7_r26r0000gn/T/ipykernel_81301/4218468640.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['block_time'] =  pd.to_datetime(df_new['block_time'], infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "df_new['block_time'] =  pd.to_datetime(df_new['block_time'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c75aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishek/opt/anaconda3/envs/itp/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(data=df_new,schema = simpleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce0ac8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 03:44:25 WARN TaskSetManager: Stage 9 contains a task of very large size (2173 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/20 03:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/20 03:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/20 03:44:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.write.parquet(\"/Users/abhishek/Documents/load/Parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
